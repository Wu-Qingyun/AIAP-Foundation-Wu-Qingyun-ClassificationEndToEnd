{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74946d34",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: HDB Resale Flats Classification\n",
    "\n",
    "This notebook analyzes the HDB resale flats dataset to understand the distribution of features and their relationships with price categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b67ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Load configuration\n",
    "try:\n",
    "    with open('../src/config.yaml', 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Configuration file not found. Please check the path.\")\n",
    "    raise\n",
    "\n",
    "# Load the raw data\n",
    "try:\n",
    "    df = pd.read_csv(Path('../') / config['data']['raw_data_path'])\n",
    "    print(f\"Data loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Data file not found. Please check the path.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d450624a",
   "metadata": {},
   "source": [
    "## 1. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nColumns:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicates}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Removed {duplicates} duplicate rows. New shape: {df.shape}\")\n",
    "\n",
    "# Convert data types where appropriate\n",
    "# Convert categorical columns to category type\n",
    "categorical_cols = ['town_name', 'flat_type', 'storey_range', 'flatm_name', 'block']\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "# Convert price_category to binary\n",
    "df['price_category'] = (df['price_category'] == 'Above Median').astype(int)\n",
    "\n",
    "# Display updated data types\n",
    "print(\"\\nUpdated Data Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d5961",
   "metadata": {},
   "source": [
    "## 2. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba72072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for numerical features\n",
    "numerical_cols = ['floor_area_sqm', 'lease_commence_date', 'remaining_lease']\n",
    "print(\"Numerical Features Statistics:\")\n",
    "print(df[numerical_cols].describe())\n",
    "\n",
    "# Check for skewness in numerical features\n",
    "print(\"\\nSkewness of Numerical Features:\")\n",
    "for col in numerical_cols:\n",
    "    skewness = df[col].skew()\n",
    "    print(f\"{col}: {skewness:.4f}\")\n",
    "    if abs(skewness) > 1:\n",
    "        print(f\"  - {col} is highly skewed (|skewness| > 1)\")\n",
    "\n",
    "# Apply log transformation to highly skewed features\n",
    "for col in numerical_cols:\n",
    "    if df[col].skew() > 1 and df[col].min() > 0:  # Only apply log to positive values\n",
    "        df[f'{col}_log'] = np.log1p(df[col])\n",
    "        print(f\"Applied log transformation to {col}\")\n",
    "\n",
    "# Descriptive statistics for categorical features\n",
    "print(\"\\nCategorical Features Statistics:\")\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n{col} value counts:\")\n",
    "        print(df[col].value_counts().head(10))\n",
    "        print(f\"Number of unique values: {df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d09fa5",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a4cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze price category distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='price_category')\n",
    "plt.title('Distribution of Price Categories')\n",
    "plt.xlabel('Price Category (0: Below Median, 1: Above Median)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Print class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "class_dist = df['price_category'].value_counts(normalize=True)\n",
    "print(class_dist)\n",
    "print(f\"Class imbalance ratio: {class_dist[1]/class_dist[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044a0ab1",
   "metadata": {},
   "source": [
    "## 4. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52af843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "print(\"Missing Values Analysis:\")\n",
    "print(missing_df[missing_df['Missing Values'] > 0])\n",
    "\n",
    "# Visualize missing values\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=missing_df.index, y='Percentage', data=missing_df[missing_df['Missing Values'] > 0])\n",
    "plt.title('Percentage of Missing Values by Feature')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Strategy for handling missing values\n",
    "print(\"\\nStrategy for handling missing values:\")\n",
    "for col in df.columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            print(f\"- {col}: Numerical feature - Will use median imputation\")\n",
    "        else:\n",
    "            print(f\"- {col}: Categorical feature - Will use mode imputation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aecbaf",
   "metadata": {},
   "source": [
    "## 5. Outlier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23866bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers using IQR method\n",
    "def detect_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column]\n",
    "    return len(outliers), lower_bound, upper_bound\n",
    "\n",
    "# Analyze outliers for numerical features\n",
    "numerical_cols = ['floor_area_sqm', 'lease_commence_date', 'remaining_lease']\n",
    "outlier_summary = []\n",
    "\n",
    "for col in numerical_cols:\n",
    "    count, lower, upper = detect_outliers(df, col)\n",
    "    outlier_summary.append({\n",
    "        'Feature': col,\n",
    "        'Outlier Count': count,\n",
    "        'Percentage': (count / len(df)) * 100,\n",
    "        'Lower Bound': lower,\n",
    "        'Upper Bound': upper\n",
    "    })\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(\"Outlier Analysis Summary:\")\n",
    "print(outlier_df)\n",
    "\n",
    "# Visualize outliers using boxplots\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.boxplot(x='price_category', y=col, data=df)\n",
    "    plt.title(f'{col} by Price Category')\n",
    "    plt.xlabel('Price Category (0: Below Median, 1: Above Median)')\n",
    "    plt.ylabel(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize outliers using violin plots\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.violinplot(x='price_category', y=col, data=df)\n",
    "    plt.title(f'{col} Distribution by Price Category')\n",
    "    plt.xlabel('Price Category (0: Below Median, 1: Above Median)')\n",
    "    plt.ylabel(col)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d73e1",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2724efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis using mutual information\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Prepare features for mutual information calculation\n",
    "X = df.drop('price_category', axis=1)\n",
    "y = df['price_category']\n",
    "\n",
    "# Convert categorical variables to numeric using label encoding\n",
    "X_encoded = X.copy()\n",
    "for col in categorical_cols:\n",
    "    if col in X_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
    "\n",
    "# Calculate mutual information\n",
    "mi_scores = mutual_info_classif(X_encoded, y)\n",
    "mi_df = pd.DataFrame({\n",
    "    'Feature': X_encoded.columns,\n",
    "    'Mutual Information': mi_scores\n",
    "})\n",
    "mi_df = mi_df.sort_values('Mutual Information', ascending=False)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Mutual Information', y='Feature', data=mi_df)\n",
    "plt.title('Feature Importance Based on Mutual Information')\n",
    "plt.xlabel('Mutual Information Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "print(mi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f6a086",
   "metadata": {},
   "source": [
    "## 7. Key Findings and Insights\n",
    "\n",
    "1. **Data Quality**:\n",
    "   - Missing values were identified and strategies for handling them were proposed\n",
    "   - Outliers were identified and quantified for numerical features\n",
    "   - Data types were appropriately converted for analysis\n",
    "\n",
    "2. **Target Variable**:\n",
    "   - The dataset shows a balanced distribution between price categories\n",
    "   - Class imbalance ratio is calculated and reported\n",
    "\n",
    "3. **Numerical Features**:\n",
    "   - Skewness was analyzed and addressed through log transformation where appropriate\n",
    "   - Outliers were identified using the IQR method\n",
    "   - Feature importance was calculated using mutual information\n",
    "\n",
    "4. **Categorical Features**:\n",
    "   - Categorical variables were properly encoded for analysis\n",
    "   - Value counts and distributions were analyzed\n",
    "   - Relationships with the target variable were examined\n",
    "\n",
    "5. **Feature Engineering Recommendations**:\n",
    "   - Features requiring log transformation were identified\n",
    "   - Important features for prediction were ranked\n",
    "   - Strategies for handling missing values were proposed\n",
    "\n",
    "These insights will guide:\n",
    "1. Feature selection and preprocessing steps\n",
    "2. Model selection and training strategies\n",
    "3. Data cleaning and transformation decisions"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
